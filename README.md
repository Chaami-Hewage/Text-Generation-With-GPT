# Text Generation with GPT-2

This repository contains code for text generation using the GPT-2 model. You can generate text based on a provided prompt using different techniques such as beam search and nucleus sampling.

## Getting Started

To run this notebook, you need to have access to a GPU runtime on Google Colab.

1. Open the notebook in Google Colab.
2. Change the runtime type to GPU.
3. Run the notebook cells one by one.

## Dependencies

Make sure you have the following dependencies installed:

- `transformers`
- `torch`

You can install them using pip:

```bash
!pip install transformers torch
